A showcase of various NLP tasks I performed while studying Natural Language Processing (USYD SOFT4446). Each task I personally did is compiled at the bottom of each notebook, a summary of each is included below:

# Lab 3
Uses the Pytorch library to build and operate a Neural Network. Primarily focuses on the role of each component of the network and what the final model actually does. 

# Lab 4
Uses Pytorch to build and operate a basic character-level Recurrent Neural Network (RNN) to classify words.

# Lab 5
Uses Tensorflow/Keras to build a Recurrent Neural Network (RNN) for sentiment analysis. Also creates an LSTM (A special type of RNN) in Keras, and explores the different types of samplers (beam, Contrast, TopK etc) and their effect on each model.  

# Lab 7
The implementation of Pytorch to translate between languages with a Sequence to Sequence network, and exploring the effect and implementation of Attention.

# Lab 8
Uses the Transformers library to explore text generation, e.g "Im going to steal the" then filling out the resot of the sentence, like ChatGPT without training it to be conversational. Also covers NER, translation and summarization. 

# Lab 10
Covers how to integrate LLM like Pinecone and ChatGPT into Python and a number of practical applications of doing so. 

# Lab 12
uses the Vowpal Wabbit library to show how NLP handles similar tasks with 'HUGE' amounts of data.
