{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k7Lc9I6taO3k"
   },
   "source": [
    "# Lab 10\n",
    "\n",
    "Today, we'll explore a vector database - Pinecone. It is based on [two tutorials](https://docs.pinecone.io/examples/notebooks) from the Pinecone developers.\n",
    "\n",
    "## Setup\n",
    "\n",
    "If you haven't already, set up accounts with Pinecone and OpenAI:\n",
    "\n",
    "Pinecone - https://app.pinecone.io\n",
    "\n",
    "Once you create an account they will ask you a few questions to get it set up. Choose Python as the language. The rest you can answer however you like.\n",
    "\n",
    "OpenAI API - https://platform.openai.com/overview\n",
    "\n",
    "Note, if you are creating a new account you should get some free credits that you can use. If you already have an account, the cost for the lab should be very small.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "bifSiBjLSKFK",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# api key from app.pinecone.io\n",
    "api_key = 'a0c11910-4718-4759-99d7-4cf8cfc48b56'\n",
    "\n",
    "# api key from platform.openai.com\n",
    "openai_api_key = 'INSERT_YOUR_OPENAI_KEY_HERE'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AgyCsGklSKNV"
   },
   "source": [
    "Now, let's install the libraries we need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "q03L1BYEZQfe",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -qU \\\n",
    "  openai==0.27.7 \\\n",
    "  pinecone-client==3.1.0 \\\n",
    "  pinecone-datasets==0.7.0 \\\n",
    "  sentence-transformers==2.2.2 \\\n",
    "  tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hrSfFiIC5roI"
   },
   "source": [
    "## Data Download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kujS_e8s55oJ"
   },
   "source": [
    "In this notebook we will use a pre-processed dataset from Pinecone Datasets.\n",
    "\n",
    "If you are curious about what pre-processing they did. see [this notebook](https://github.com/pinecone-io/examples/blob/master/learn/search/semantic-search/semantic-search.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "lOgjRG52Zqqz",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>values</th>\n",
       "      <th>sparse_values</th>\n",
       "      <th>metadata</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>240000</th>\n",
       "      <td>515997</td>\n",
       "      <td>[-0.00531694, 0.06937869, -0.0092854, 0.003286...</td>\n",
       "      <td>{'indices': [845, 1657, 13677, 20780, 27058, 2...</td>\n",
       "      <td>{'text': ' Why is a \"law of sciences\" importan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240001</th>\n",
       "      <td>515998</td>\n",
       "      <td>[-0.09243751, 0.065432355, -0.06946959, 0.0669...</td>\n",
       "      <td>{'indices': [2110, 6324, 9754, 13677, 15207, 2...</td>\n",
       "      <td>{'text': ' Is it possible to format a BitLocke...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240002</th>\n",
       "      <td>515999</td>\n",
       "      <td>[-0.021924071, 0.032280188, -0.020190848, 0.07...</td>\n",
       "      <td>{'indices': [2110, 4949, 23579, 23758, 27058, ...</td>\n",
       "      <td>{'text': ' Can formatting a hard drive stress ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240003</th>\n",
       "      <td>516000</td>\n",
       "      <td>[-0.120020054, 0.024080949, 0.10693012, -0.018...</td>\n",
       "      <td>{'indices': [22014, 24734, 24773, 25791, 25991...</td>\n",
       "      <td>{'text': ' Are the new Samsung Galaxy J7 and J...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240004</th>\n",
       "      <td>516001</td>\n",
       "      <td>[-0.095293395, -0.048446465, -0.017618902, -0....</td>\n",
       "      <td>{'indices': [307, 2110, 5785, 12969, 12971, 13...</td>\n",
       "      <td>{'text': ' I just watched an add for Indonesia...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                                             values  \\\n",
       "240000  515997  [-0.00531694, 0.06937869, -0.0092854, 0.003286...   \n",
       "240001  515998  [-0.09243751, 0.065432355, -0.06946959, 0.0669...   \n",
       "240002  515999  [-0.021924071, 0.032280188, -0.020190848, 0.07...   \n",
       "240003  516000  [-0.120020054, 0.024080949, 0.10693012, -0.018...   \n",
       "240004  516001  [-0.095293395, -0.048446465, -0.017618902, -0....   \n",
       "\n",
       "                                            sparse_values  \\\n",
       "240000  {'indices': [845, 1657, 13677, 20780, 27058, 2...   \n",
       "240001  {'indices': [2110, 6324, 9754, 13677, 15207, 2...   \n",
       "240002  {'indices': [2110, 4949, 23579, 23758, 27058, ...   \n",
       "240003  {'indices': [22014, 24734, 24773, 25791, 25991...   \n",
       "240004  {'indices': [307, 2110, 5785, 12969, 12971, 13...   \n",
       "\n",
       "                                                 metadata  \n",
       "240000  {'text': ' Why is a \"law of sciences\" importan...  \n",
       "240001  {'text': ' Is it possible to format a BitLocke...  \n",
       "240002  {'text': ' Can formatting a hard drive stress ...  \n",
       "240003  {'text': ' Are the new Samsung Galaxy J7 and J...  \n",
       "240004  {'text': ' I just watched an add for Indonesia...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pinecone_datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('quora_all-MiniLM-L6-bm25')\n",
    "\n",
    "# we drop metadata as will use blob column\n",
    "dataset.documents.drop(['metadata'], axis=1, inplace=True)\n",
    "dataset.documents.rename(columns={'blob': 'metadata'}, inplace=True)\n",
    "\n",
    "# we will use 80K rows of the dataset between rows 240K -> 320K\n",
    "dataset.documents.drop(dataset.documents.index[320_000:], inplace=True)\n",
    "dataset.documents.drop(dataset.documents.index[:240_000], inplace=True)\n",
    "\n",
    "# Print out a sample from the dataset to show what we are working with\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "conuh2Uo-mwR",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80000\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ebd7XSamfMsC"
   },
   "source": [
    "## Creating an Index\n",
    "\n",
    "Now the data is ready, we can set up our index to store it.\n",
    "\n",
    "We begin by initializing our connection to Pinecone. This is where your API key is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "mc66NEBAcQHY",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pinecone import Pinecone\n",
    "\n",
    "# configure client\n",
    "pc = Pinecone(api_key=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xknsDRh15-by"
   },
   "source": [
    "Now we set up our index specification. This allows us to define the cloud provider and region where we want to deploy our index. You can find a list of all available providers and regions [here](https://docs.pinecone.io/docs/projects)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "m0BUsx0E590t",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pinecone import ServerlessSpec\n",
    "\n",
    "cloud = os.environ.get('PINECONE_CLOUD') or 'aws'\n",
    "region = os.environ.get('PINECONE_REGION') or 'us-east-1'\n",
    "\n",
    "spec = ServerlessSpec(cloud=cloud, region=region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SdaTip6CfllN"
   },
   "source": [
    "Now we create a new index called `semantic-search-fast`. It's important that we align the index `dimension` and `metric` parameters with those required by the `MiniLM-L6` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "kT8pfoO46Iwg",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 384,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {},\n",
       " 'total_vector_count': 0}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "index_name = 'semantic-search-fast'\n",
    "\n",
    "existing_indexes = [\n",
    "    index_info[\"name\"] for index_info in pc.list_indexes()\n",
    "]\n",
    "\n",
    "# check if index already exists (it shouldn't if this is first time)\n",
    "if index_name not in existing_indexes:\n",
    "    # if does not exist, create index\n",
    "    pc.create_index(\n",
    "        index_name,\n",
    "        dimension=384,  # dimensionality of minilm\n",
    "        metric='dotproduct',\n",
    "        spec=spec\n",
    "    )\n",
    "    # wait for index to be initialized\n",
    "    while not pc.describe_index(index_name).status['ready']:\n",
    "        time.sleep(1)\n",
    "\n",
    "# connect to index\n",
    "index = pc.Index(index_name)\n",
    "time.sleep(1)\n",
    "# view index stats\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YUd1VGg6i108"
   },
   "source": [
    "Upsert the data to put it in the database (this can take 2-5 minutes):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "RhR6WOi1huXZ",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e18aad4e01954c99a0c65883209533de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/160 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "for batch in tqdm(dataset.iter_documents(batch_size=500), total=160):\n",
    "    index.upsert(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VrK_IN079Vuu"
   },
   "source": [
    "## Making Queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rr4unPAq9alb"
   },
   "source": [
    "Now that our index is populated we can begin making queries. We are performing a semantic search for *similar questions*, so we should embed and search with another question.\n",
    "\n",
    "Note that we use the same model as the one used above. That's critical - otherwise the vector spaces will not be meaningfully comparable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Fqo_hMRZiubM",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b1a855e79e94b62bc1080ad8d2c4c29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".gitattributes:   0%|          | 0.00/1.23k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f47b9b860c843e4a31b65a579b2d490",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "012869e1fda942ac9acc14b7c1261d5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e217925b33df4754bdf8b0989a9b36e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "954df1b9501147f5955a237c55ff86fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43d29ec406264954a3c5220aedb0157c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data_config.json:   0%|          | 0.00/39.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1afdb7f000ac4c2399d3cc864c9a03bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ba9eec3911849929597e5c449b5d2ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ee8d0eea3cf4ea9aa7a24dae18f0482",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bccdedf4ac7648c6a9b6e0a8ca87524c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08afe5d116f1435ca3b36edbae0e39e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe1785740b0b4f94921df20bdcdbaa7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0967d91e672848919e77bce803b7d677",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train_script.py:   0%|          | 0.00/13.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51b7ecf3a35942be805cd9cc20f51c8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88891b8587f84c6f9c43d178d9bc840c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \n",
       "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n",
       "  (2): Normalize()\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2', device=device)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MP2-unZ--XJ9"
   },
   "source": [
    "Now let's query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "JWcO7jAK-N_1",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'matches': [{'id': '69331',\n",
       "              'metadata': {'text': \" What's the world's largest city?\"},\n",
       "              'score': 0.785655141,\n",
       "              'values': []},\n",
       "             {'id': '69332',\n",
       "              'metadata': {'text': ' What is the biggest city?'},\n",
       "              'score': 0.727139473,\n",
       "              'values': []},\n",
       "             {'id': '84749',\n",
       "              'metadata': {'text': \" What are the world's most advanced \"\n",
       "                                   'cities?'},\n",
       "              'score': 0.709211409,\n",
       "              'values': []},\n",
       "             {'id': '109231',\n",
       "              'metadata': {'text': ' Where is the most beautiful city in the '\n",
       "                                   'world?'},\n",
       "              'score': 0.696054876,\n",
       "              'values': []},\n",
       "             {'id': '109230',\n",
       "              'metadata': {'text': ' What is the greatest, most beautiful city '\n",
       "                                   'in the world?'},\n",
       "              'score': 0.657444596,\n",
       "              'values': []}],\n",
       " 'namespace': '',\n",
       " 'usage': {'read_units': 6}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"which city has the highest population in the world?\"\n",
    "\n",
    "# create the query vector\n",
    "xq = model.encode(query).tolist()\n",
    "\n",
    "# now query\n",
    "xc = index.query(vector=xq, top_k=5, include_metadata=True)\n",
    "xc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-XwOWcgo_QtI"
   },
   "source": [
    "In the returned response `xc` we can see the most relevant questions to our particular query — we don't have any exact matches but we can see that the returned questions are similar in the topics they are asking about. We can reformat this response to be a little easier to read:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "gy7isg_f-vWg",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.79:  What's the world's largest city?\n",
      "0.73:  What is the biggest city?\n",
      "0.71:  What are the world's most advanced cities?\n",
      "0.7:  Where is the most beautiful city in the world?\n",
      "0.66:  What is the greatest, most beautiful city in the world?\n"
     ]
    }
   ],
   "source": [
    "for result in xc['matches']:\n",
    "    print(f\"{round(result['score'], 2)}: {result['metadata']['text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1JK5yApl_5fE"
   },
   "source": [
    "These are good results, let's try and modify the words being used to see if we still surface similar results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "dJbjE-iq_yMr",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.64:  What is the biggest city?\n",
      "0.6:  What is the most dangerous city in USA?\n",
      "0.59:  What's the world's largest city?\n",
      "0.59:  What is the most dangerous city in USA? Why?\n",
      "0.58:  What are the world's most advanced cities?\n"
     ]
    }
   ],
   "source": [
    "query = \"which metropolis has the highest number of people?\"\n",
    "\n",
    "# create the query vector\n",
    "xq = model.encode(query).tolist()\n",
    "\n",
    "# now query\n",
    "xc = index.query(vector=xq, top_k=5, include_metadata=True)\n",
    "for result in xc['matches']:\n",
    "    print(f\"{round(result['score'], 2)}: {result['metadata']['text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HIAxOPb-A2w_"
   },
   "source": [
    "Here we used different terms in our query than that of the returned documents. We substituted **\"city\"** for **\"metropolis\"** and **\"populated\"** for **\"number of people\"**.\n",
    "\n",
    "Despite these very different terms and *lack* of term overlap between query and returned documents — we get highly relevant results — this is the power of *semantic search*.\n",
    "\n",
    "## Task 1\n",
    "\n",
    "Try changing the model and querying again. You can find alternative models [here](https://sbert.net/docs/pretrained_models.html). Note that you will need to choose one with the same dimensionality (384). Clicking on the \"info\" symbol next to the model names will tell you information including their dimensionality.\n",
    "\n",
    "Find a model that gives similar results and a model that gives different results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "-cWdeKzhAtww",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.62:  What buildings do other countries people live?\n",
      "1.46:  Quora, Reddit, 4chan, and tumblr are now their own countries. What does life look like in each?\n",
      "1.45:  How do the people without high incomes live in San Francisco?\n",
      "1.45:  Where can I live totally alone without having to pay taxes or talk to people?\n",
      "1.44:  How is the life of Indians who settle abroad (US, UK, Canada, Australia or New Zealand) ?\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "model = SentenceTransformer('sentence-transformers/paraphrase-MiniLM-L3-v2', device=device)\n",
    "\n",
    "query = \"Where do people live?\"\n",
    "\n",
    "# create the query vector\n",
    "xq = model.encode(query).tolist()\n",
    "\n",
    "# now query\n",
    "xc = index.query(vector=xq, top_k=5, include_metadata=True)\n",
    "for result in xc['matches']:\n",
    "    print(f\"{round(result['score'], 2)}: {result['metadata']['text']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U1oqGm36J94S"
   },
   "source": [
    "# Retrieval Enhanced Generative Question Answering\n",
    "\n",
    "Next, we will see how these queries can be used with an LLM to generate better outputs.\n",
    "\n",
    "We will again use data that has already been prepared (for details, see [this notebook](https://github.com/pinecone-io/examples/blob/master/learn/generation/openai/gen-qa-openai.ipynb))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nX_fy-BrJ-t1"
   },
   "outputs": [],
   "source": [
    "from pinecone_datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('youtube-transcripts-text-embedding-ada-002')\n",
    "\n",
    "# we drop sparse_values as they are not needed for this example\n",
    "dataset.documents.drop(['metadata'], axis=1, inplace=True)\n",
    "dataset.documents.rename(columns={'blob': 'metadata'}, inplace=True)\n",
    "\n",
    "# Print a sample of the data\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R4KY86YXKBmA"
   },
   "source": [
    "Again, we will set up a pinecone database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7cVhu-tGKJGH"
   },
   "outputs": [],
   "source": [
    "index_name = 'gen-qa-openai-fast'\n",
    "# check if index already exists (it shouldn't if this is first time)\n",
    "if index_name not in pc.list_indexes().names():\n",
    "    # if does not exist, create index\n",
    "    pc.create_index(\n",
    "        index_name,\n",
    "        dimension=1536,  # dimensionality of text-embedding-ada-002\n",
    "        metric='cosine',\n",
    "        spec=spec\n",
    "    )\n",
    "# connect to index\n",
    "index = pc.Index(index_name)\n",
    "# view index stats\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "liE7x3RiKLXP"
   },
   "source": [
    "As in the previous section, we'll insert the data into the database (this can take 5-10 minutes):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Mrp_cGJKMfB"
   },
   "outputs": [],
   "source": [
    "for batch in dataset.iter_documents(batch_size=100):\n",
    "    index.upsert(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H356HGpnKM3P"
   },
   "source": [
    "Now we've added all of our langchain docs to the index. With that we can move on to retrieval and then answer generation.\n",
    "\n",
    "## Retrieval\n",
    "\n",
    "To search through our documents we first need to create a query vector `xq`. Using `xq` we will retrieve the most relevant chunks from the LangChain docs. To create that query vector we must initialize a `text-embedding-ada-002` embedding model with OpenAI. For this, you need an [OpenAI API key](https://platform.openai.com/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o9m3xdRXKRqA"
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "openai.api_key = openai_api_key\n",
    "\n",
    "embed_model = \"text-embedding-ada-002\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BKqh91UyKT4D"
   },
   "outputs": [],
   "source": [
    "query = (\n",
    "    \"Which training method should I use for sentence transformers when \" +\n",
    "    \"I only have pairs of related sentences?\"\n",
    ")\n",
    "\n",
    "res = openai.Embedding.create(\n",
    "    input=[query],\n",
    "    engine=embed_model\n",
    ")\n",
    "\n",
    "# retrieve from Pinecone\n",
    "xq = res['data'][0]['embedding']\n",
    "\n",
    "# get relevant contexts (including the questions)\n",
    "res = index.query(vector=xq, top_k=2, include_metadata=True)\n",
    "\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MA7wEDhnKWim"
   },
   "source": [
    "We write some functions to handle the retrieval and completion steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UlwUDNbBKXh2"
   },
   "outputs": [],
   "source": [
    "limit = 3750\n",
    "\n",
    "import time\n",
    "\n",
    "def retrieve(query):\n",
    "    res = openai.Embedding.create(\n",
    "        input=[query],\n",
    "        engine=embed_model\n",
    "    )\n",
    "\n",
    "    # retrieve from Pinecone\n",
    "    xq = res['data'][0]['embedding']\n",
    "\n",
    "    # get relevant contexts\n",
    "    contexts = []\n",
    "    time_waited = 0\n",
    "    while (len(contexts) < 3 and time_waited < 60 * 12):\n",
    "        res = index.query(vector=xq, top_k=3, include_metadata=True)\n",
    "        contexts = contexts + [\n",
    "            x['metadata']['text'] for x in res['matches']\n",
    "        ]\n",
    "        print(f\"Retrieved {len(contexts)} contexts, sleeping for 15 seconds...\")\n",
    "        time.sleep(15)\n",
    "        time_waited += 15\n",
    "\n",
    "    if time_waited >= 60 * 12:\n",
    "        print(\"Timed out waiting for contexts to be retrieved.\")\n",
    "        contexts = [\"No contexts retrieved. Try to answer the question yourself!\"]\n",
    "\n",
    "\n",
    "    # build our prompt with the retrieved contexts included\n",
    "    prompt_start = (\n",
    "        \"Answer the question based on the context below.\\n\\n\"+\n",
    "        \"Context:\\n\"\n",
    "    )\n",
    "    prompt_end = (\n",
    "        f\"\\n\\nQuestion: {query}\\nAnswer:\"\n",
    "    )\n",
    "    # append contexts until hitting limit\n",
    "    for i in range(1, len(contexts)):\n",
    "        if len(\"\\n\\n---\\n\\n\".join(contexts[:i])) >= limit:\n",
    "            prompt = (\n",
    "                prompt_start +\n",
    "                \"\\n\\n---\\n\\n\".join(contexts[:i-1]) +\n",
    "                prompt_end\n",
    "            )\n",
    "            break\n",
    "        elif i == len(contexts)-1:\n",
    "            prompt = (\n",
    "                prompt_start +\n",
    "                \"\\n\\n---\\n\\n\".join(contexts) +\n",
    "                prompt_end\n",
    "            )\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def complete(prompt):\n",
    "    # instructions\n",
    "    sys_prompt = \"You are a helpful assistant that always answers questions.\"\n",
    "    # query text-davinci-003\n",
    "    res = openai.ChatCompletion.create(\n",
    "        model='gpt-3.5-turbo-0613',\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": sys_prompt},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "    return res['choices'][0]['message']['content'].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2ZgLxE_FKY3L"
   },
   "outputs": [],
   "source": [
    "# first we retrieve relevant items from Pinecone\n",
    "query_with_contexts = retrieve(query)\n",
    "query_with_contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Qjw1q7-KaW-"
   },
   "outputs": [],
   "source": [
    "# then we complete the context-infused query\n",
    "complete(query_with_contexts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q91Wd5GNKb_A"
   },
   "source": [
    "And we get a pretty great answer straight away, specifying to use _multiple-rankings loss_ (also called _multiple negatives ranking loss_).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z3J4bw3JKgUp"
   },
   "source": [
    "## Task 2 [Optional]\n",
    "\n",
    "Try adjusting the number of contexts down to 1, to see the impact on retrieval quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OPSUDTWJKgkX"
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y82uI1BDKg4N"
   },
   "source": [
    "## Pack up\n",
    "\n",
    "Once you're done with the lab, delete the indices to save resources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iw8DMgvyKe33"
   },
   "outputs": [],
   "source": [
    "pc.delete_index('gen-qa-openai-fast')\n",
    "pc.delete_index('semantic-search-fast')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
