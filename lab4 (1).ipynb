{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4\n",
    "\n",
    "We will be building and training a basic character-level Recurrent Neural\n",
    "Network (RNN) to classify words. This lab is based on Based on [\"NLP From Scratch: Classifying Names with a Character-Level RNN\"](https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html) by [Sean Robertson](https://github.com/spro).\n",
    "\n",
    "A character-level RNN reads words as a series of characters and for each one it (a) produces an output, and (b) updates a hidden state vector. The output hidden state from one step is in the input to the next step. In this lab, the final prediction will be made based on the last output.\n",
    "\n",
    "The task we'll consider is predicting the language of origin of a name.\n",
    "We'll train on a few thousand surnames from 18 languages\n",
    "of origin, and predict which language a name is from based on the\n",
    "spelling. For example:\n",
    "\n",
    "```sh\n",
    "$ python predict.py Hinton\n",
    "(-0.47) Scottish\n",
    "(-1.52) English\n",
    "(-3.57) Irish\n",
    "\n",
    "$ python predict.py Schmidhuber\n",
    "(-0.19) German\n",
    "(-2.48) Czech\n",
    "(-2.68) Dutch\n",
    "```\n",
    "\n",
    "\n",
    "## Download the Data\n",
    "\n",
    "Download the data and extract it:\n",
    "\n",
    "1. Open a terminal (purple + button in the top left, then select Terminal in the bottom row)\n",
    "2. Run:\n",
    "\n",
    "```\n",
    "wget https://download.pytorch.org/tutorial/data.zip\n",
    "unzip data.zip\n",
    "```\n",
    "\n",
    "## Preparing the Data\n",
    "\n",
    "Included in the ``data/names`` directory are 18 text files named as\n",
    "``[Language].txt``. Each file contains a bunch of names, one name per\n",
    "line, mostly romanized (but we still need to convert from Unicode to\n",
    "ASCII).\n",
    "\n",
    "We'll end up with a dictionary of lists of names per language,\n",
    "``{language: [names ...]}``. The generic variables \"category\" and \"line\"\n",
    "(for language and name in our case) are used for later extensibility.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files found:\n",
      "['data/names/Arabic.txt', 'data/names/Chinese.txt', 'data/names/Czech.txt', 'data/names/Dutch.txt', 'data/names/English.txt', 'data/names/French.txt', 'data/names/German.txt', 'data/names/Greek.txt', 'data/names/Irish.txt', 'data/names/Italian.txt', 'data/names/Japanese.txt', 'data/names/Korean.txt', 'data/names/Polish.txt', 'data/names/Portuguese.txt', 'data/names/Russian.txt', 'data/names/Scottish.txt', 'data/names/Spanish.txt', 'data/names/Vietnamese.txt']\n",
      "\n",
      "Example conversion from Unicode to ASCII:\n",
      "Input: Ślusàrski\n",
      "Output: Slusarski\n"
     ]
    }
   ],
   "source": [
    "from io import open\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Get files\n",
    "def findFiles(path):\n",
    "    return glob.glob(path)\n",
    "\n",
    "print(\"Files found:\")\n",
    "print(findFiles('data/names/*.txt'))\n",
    "\n",
    "# Turn a Unicode string to plain ASCII, thanks to https://stackoverflow.com/a/518232/2809427\n",
    "\n",
    "import unicodedata\n",
    "import string\n",
    "\n",
    "all_letters = string.ascii_letters + \" .,;'\"\n",
    "n_letters = len(all_letters)\n",
    "\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in all_letters\n",
    "    )\n",
    "\n",
    "print(\"\\nExample conversion from Unicode to ASCII:\")\n",
    "print(\"Input: Ślusàrski\")\n",
    "print(\"Output:\", unicodeToAscii('Ślusàrski'))\n",
    "\n",
    "# Build the category_lines dictionary, a list of names per language\n",
    "category_lines = {}\n",
    "all_categories = []\n",
    "\n",
    "# Read a file and split into lines\n",
    "def readLines(filename):\n",
    "    lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
    "    return [unicodeToAscii(line) for line in lines]\n",
    "\n",
    "for filename in findFiles('data/names/*.txt'):\n",
    "    category = os.path.splitext(os.path.basename(filename))[0]\n",
    "    all_categories.append(category)\n",
    "    lines = readLines(filename)\n",
    "    category_lines[category] = lines\n",
    "\n",
    "n_categories = len(all_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have ``category_lines``, a dictionary mapping each category\n",
    "(language) to a list of lines (names). We also kept track of\n",
    "``all_categories`` (just a list of languages) and ``n_categories`` for\n",
    "later reference.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Abandonato', 'Abatangelo', 'Abatantuono', 'Abate', 'Abategiovanni', 'Abatescianni', 'Abba', 'Abbadelli', 'Abbascia', 'Abbatangelo', 'Abbatantuono', 'Abbate', 'Abbatelli', 'Abbaticchio', 'Abbiati', 'Abbracciabene', 'Abbracciabeni', 'Abelli', 'Abello', 'Abrami', 'Abramo', 'Acardi', 'Accardi', 'Accardo', 'Acciai', 'Acciaio', 'Acciaioli', 'Acconci', 'Acconcio', 'Accorsi', 'Accorso', 'Accosi', 'Accursio', 'Acerbi', 'Acone', 'Aconi', 'Acqua', 'Acquafredda', 'Acquarone', 'Acquati', 'Adalardi', 'Adami', 'Adamo', 'Adamoli', 'Addario', 'Adelardi', 'Adessi', 'Adimari', 'Adriatico', 'Affini', 'Africani', 'Africano', 'Agani', 'Aggi', 'Aggio', 'Agli', 'Agnelli', 'Agnellutti', 'Agnusdei', 'Agosti', 'Agostini', 'Agresta', 'Agrioli', 'Aiello', 'Aiolfi', 'Airaldi', 'Airo', 'Aita', 'Ajello', 'Alagona', 'Alamanni', 'Albanesi', 'Albani', 'Albano', 'Alberghi', 'Alberghini', 'Alberici', 'Alberighi', 'Albero', 'Albini', 'Albricci', 'Albrici', 'Alcheri', 'Aldebrandi', 'Alderisi', 'Alduino', 'Alemagna', 'Aleppo', 'Alesci', 'Alescio', 'Alesi', 'Alesini', 'Alesio', 'Alessandri', 'Alessi', 'Alfero', 'Aliberti', 'Alinari', 'Aliprandi', 'Allegri', 'Allegro', 'Alo', 'Aloia', 'Aloisi', 'Altamura', 'Altimari', 'Altoviti', 'Alunni', 'Amadei', 'Amadori', 'Amalberti', 'Amantea', 'Amato', 'Amatore', 'Ambrogi', 'Ambrosi', 'Amello', 'Amerighi', 'Amoretto', 'Angioli', 'Ansaldi', 'Anselmetti', 'Anselmi', 'Antonelli', 'Antonini', 'Antonino', 'Aquila', 'Aquino', 'Arbore', 'Ardiccioni', 'Ardizzone', 'Ardovini', 'Arena', 'Aringheri', 'Arlotti', 'Armani', 'Armati', 'Armonni', 'Arnolfi', 'Arnoni', 'Arrighetti', 'Arrighi', 'Arrigucci', 'Aucciello', 'Azzara', 'Baggi', 'Baggio', 'Baglio', 'Bagni', 'Bagnoli', 'Balboni', 'Baldi', 'Baldini', 'Baldinotti', 'Baldovini', 'Bandini', 'Bandoni', 'Barbieri', 'Barone', 'Barsetti', 'Bartalotti', 'Bartolomei', 'Bartolomeo', 'Barzetti', 'Basile', 'Bassanelli', 'Bassani', 'Bassi', 'Basso', 'Basurto', 'Battaglia', 'Bazzoli', 'Bellandi', 'Bellandini', 'Bellincioni', 'Bellini', 'Bello', 'Bellomi', 'Belloni', 'Belluomi', 'Belmonte', 'Bencivenni', 'Benedetti', 'Benenati', 'Benetton', 'Benini', 'Benivieni', 'Benvenuti', 'Berardi', 'Bergamaschi', 'Berti', 'Bertolini', 'Biancardi', 'Bianchi', 'Bicchieri', 'Biondi', 'Biondo', 'Boerio', 'Bologna', 'Bondesan', 'Bonomo', 'Borghi', 'Borgnino', 'Borgogni', 'Bosco', 'Bove', 'Bover', 'Boveri', 'Brambani', 'Brambilla', 'Breda', 'Brioschi', 'Brivio', 'Brunetti', 'Bruno', 'Buffone', 'Bulgarelli', 'Bulgari', 'Buonarroti', 'Busto', 'Caiazzo', 'Caito', 'Caivano', 'Calabrese', 'Calligaris', 'Campana', 'Campo', 'Cantu', 'Capello', 'Capello', 'Capello', 'Capitani', 'Carbone', 'Carboni', 'Carideo', 'Carlevaro', 'Caro', 'Carracci', 'Carrara', 'Caruso', 'Cassano', 'Castro', 'Catalano', 'Cattaneo', 'Cavalcante', 'Cavallo', 'Cingolani', 'Cino', 'Cipriani', 'Cisternino', 'Coiro', 'Cola', 'Colombera', 'Colombo', 'Columbo', 'Como', 'Como', 'Confortola', 'Conti', 'Corna', 'Corti', 'Corvi', 'Costa', 'Costantini', 'Costanzo', 'Cracchiolo', 'Cremaschi', 'Cremona', 'Cremonesi', 'Crespo', 'Croce', 'Crocetti', 'Cucinotta', 'Cuocco', 'Cuoco', \"D'ambrosio\", 'Damiani', \"D'amore\", \"D'angelo\", \"D'antonio\", 'De angelis', 'De campo', 'De felice', 'De filippis', 'De fiore', 'De laurentis', 'De luca', 'De palma', 'De rege', 'De santis', 'De vitis', 'Di antonio', 'Di caprio', 'Di mercurio', 'Dinapoli', 'Dioli', 'Di pasqua', 'Di pietro', 'Di stefano', 'Donati', \"D'onofrio\", 'Drago', 'Durante', 'Elena', 'Episcopo', 'Ermacora', 'Esposito', 'Evangelista', 'Fabbri', 'Fabbro', 'Falco', 'Faraldo', 'Farina', 'Farro', 'Fattore', 'Fausti', 'Fava', 'Favero', 'Fermi', 'Ferrara', 'Ferrari', 'Ferraro', 'Ferrero', 'Ferro', 'Fierro', 'Filippi', 'Fini', 'Fiore', 'Fiscella', 'Fiscella', 'Fonda', 'Fontana', 'Fortunato', 'Franco', 'Franzese', 'Furlan', 'Gabrielli', 'Gagliardi', 'Gallo', 'Ganza', 'Garfagnini', 'Garofalo', 'Gaspari', 'Gatti', 'Genovese', 'Gentile', 'Germano', 'Giannino', 'Gimondi', 'Giordano', 'Gismondi', 'Giugovaz', 'Giunta', 'Goretti', 'Gori', 'Greco', 'Grillo', 'Grimaldi', 'Gronchi', 'Guarneri', 'Guerra', 'Guerriero', 'Guidi', 'Guttuso', 'Idoni', 'Innocenti', 'Labriola', 'Laconi', 'Lagana', 'Lagomarsino', 'Lagorio', 'Laguardia', 'Lama', 'Lamberti', 'Lamon', 'Landi', 'Lando', 'Landolfi', 'Laterza', 'Laurito', 'Lazzari', 'Lecce', 'Leccese', 'Leggieri', 'Lemmi', 'Leone', 'Leoni', 'Lippi', 'Locatelli', 'Lombardi', 'Longo', 'Lupo', 'Luzzatto', 'Maestri', 'Magro', 'Mancini', 'Manco', 'Mancuso', 'Manfredi', 'Manfredonia', 'Mantovani', 'Marchegiano', 'Marchesi', 'Marchetti', 'Marchioni', 'Marconi', 'Mari', 'Maria', 'Mariani', 'Marino', 'Marmo', 'Martelli', 'Martinelli', 'Masi', 'Masin', 'Mazza', 'Merlo', 'Messana', 'Micheli', 'Milani', 'Milano', 'Modugno', 'Mondadori', 'Mondo', 'Montagna', 'Montana', 'Montanari', 'Monte', 'Monti', 'Morandi', 'Morello', 'Moretti', 'Morra', 'Moschella', 'Mosconi', 'Motta', 'Muggia', 'Muraro', 'Murgia', 'Murtas', 'Nacar', 'Naggi', 'Naggia', 'Naldi', 'Nana', 'Nani', 'Nanni', 'Nannini', 'Napoleoni', 'Napoletani', 'Napoliello', 'Nardi', 'Nardo', 'Nardovino', 'Nasato', 'Nascimbene', 'Nascimbeni', 'Natale', 'Nave', 'Nazario', 'Necchi', 'Negri', 'Negrini', 'Nelli', 'Nenci', 'Nepi', 'Neri', 'Neroni', 'Nervetti', 'Nervi', 'Nespola', 'Nicastro', 'Nicchi', 'Nicodemo', 'Nicolai', 'Nicolosi', 'Nicosia', 'Nicotera', 'Nieddu', 'Nieri', 'Nigro', 'Nisi', 'Nizzola', 'Noschese', 'Notaro', 'Notoriano', 'Oberti', 'Oberto', 'Ongaro', 'Orlando', 'Orsini', 'Pace', 'Padovan', 'Padovano', 'Pagani', 'Pagano', 'Palladino', 'Palmisano', 'Palumbo', 'Panzavecchia', 'Parisi', 'Parma', 'Parodi', 'Parri', 'Parrino', 'Passerini', 'Pastore', 'Paternoster', 'Pavesi', 'Pavone', 'Pavoni', 'Pecora', 'Pedrotti', 'Pellegrino', 'Perugia', 'Pesaresi', 'Pesaro', 'Pesce', 'Petri', 'Pherigo', 'Piazza', 'Piccirillo', 'Piccoli', 'Pierno', 'Pietri', 'Pini', 'Piovene', 'Piraino', 'Pisani', 'Pittaluga', 'Poggi', 'Poggio', 'Poletti', 'Pontecorvo', 'Portelli', 'Porto', 'Portoghese', 'Potenza', 'Pozzi', 'Profeta', 'Prosdocimi', 'Provenza', 'Provenzano', 'Pugliese', 'Quaranta', 'Quattrocchi', 'Ragno', 'Raimondi', 'Rais', 'Rana', 'Raneri', 'Rao', 'Rapallino', 'Ratti', 'Ravenna', 'Re', 'Ricchetti', 'Ricci', 'Riggi', 'Righi', 'Rinaldi', 'Riva', 'Rizzo', 'Robustelli', 'Rocca', 'Rocchi', 'Rocco', 'Roma', 'Roma', 'Romagna', 'Romagnoli', 'Romano', 'Romano', 'Romero', 'Roncalli', 'Ronchi', 'Rosa', 'Rossi', 'Rossini', 'Rotolo', 'Rovigatti', 'Ruggeri', 'Russo', 'Rustici', 'Ruzzier', 'Sabbadin', 'Sacco', 'Sala', 'Salomon', 'Salucci', 'Salvaggi', 'Salvai', 'Salvail', 'Salvatici', 'Salvay', 'Sanna', 'Sansone', 'Santini', 'Santoro', 'Sapienti', 'Sarno', 'Sarti', 'Sartini', 'Sarto', 'Savona', 'Scarpa', 'Scarsi', 'Scavo', 'Sciacca', 'Sciacchitano', 'Sciarra', 'Scordato', 'Scotti', 'Scutese', 'Sebastiani', 'Sebastino', 'Segreti', 'Selmone', 'Selvaggio', 'Serafin', 'Serafini', 'Serpico', 'Sessa', 'Sgro', 'Siena', 'Silvestri', 'Sinagra', 'Sinagra', 'Soldati', 'Somma', 'Sordi', 'Soriano', 'Sorrentino', 'Spada', 'Spano', 'Sparacello', 'Speziale', 'Spini', 'Stabile', 'Stablum', 'Stilo', 'Sultana', 'Tafani', 'Tamaro', 'Tamboia', 'Tanzi', 'Tarantino', 'Taverna', 'Tedesco', 'Terranova', 'Terzi', 'Tessaro', 'Testa', 'Tiraboschi', 'Tivoli', 'Todaro', 'Toloni', 'Tornincasa', 'Toselli', 'Tosetti', 'Tosi', 'Tosto', 'Trapani', 'Traversa', 'Traversi', 'Traversini', 'Traverso', 'Trucco', 'Trudu', 'Tumicelli', 'Turati', 'Turchi', 'Uberti', 'Uccello', 'Uggeri', 'Ughi', 'Ungaretti', 'Ungaro', 'Vacca', 'Vaccaro', 'Valenti', 'Valentini', 'Valerio', 'Varano', 'Ventimiglia', 'Ventura', 'Verona', 'Veronesi', 'Vescovi', 'Vespa', 'Vestri', 'Vicario', 'Vico', 'Vigo', 'Villa', 'Vinci', 'Vinci', 'Viola', 'Vitali', 'Viteri', 'Voltolini', 'Zambrano', 'Zanetti', 'Zangari', 'Zappa', 'Zeni', 'Zini', 'Zino', 'Zunino']\n"
     ]
    }
   ],
   "source": [
    "print(category_lines['Italian'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turning Names into Tensors\n",
    "\n",
    "Now that we have all the names organized, we need to turn them into\n",
    "Tensors to make any use of them.\n",
    "\n",
    "To represent a single letter, we use a one-hot vector of size\n",
    "`<1 x n_letters>`. A one-hot vector is filled with 0s except for a 1\n",
    "at index of the current letter, e.g. `\"b\" = <0 1 0 0 0 ...>`.\n",
    "In lecture 1, we noted that usually we use special data structures to avoid memory overhead (e.g., dictionaries or sparse vectors).\n",
    "In this lab, we'll use a normal vector for convenience.\n",
    "\n",
    "To make a word we join a bunch of those into a 2D matrix\n",
    "``<line_length x 1 x n_letters>``.\n",
    "\n",
    "That extra 1 dimension is because PyTorch assumes everything is in\n",
    "batches - we're just using a batch size of 1 here.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the tensor for 'J':\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0.]])\n",
      "\n",
      "This is the dimensionality of the matrix for 'Jones':\n",
      "torch.Size([5, 1, 57])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Find letter index from all_letters, e.g. \"a\" = 0\n",
    "def letterToIndex(letter):\n",
    "    return all_letters.find(letter)\n",
    "\n",
    "# Just for demonstration, turn a letter into a <1 x n_letters> Tensor\n",
    "def letterToTensor(letter):\n",
    "    tensor = torch.zeros(1, n_letters)\n",
    "    tensor[0][letterToIndex(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "# Turn a line into a <line_length x 1 x n_letters>,\n",
    "# or an array of one-hot letter vectors\n",
    "def lineToTensor(line):\n",
    "    tensor = torch.zeros(len(line), 1, n_letters)\n",
    "    for li, letter in enumerate(line):\n",
    "        tensor[li][0][letterToIndex(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "print(\"This is the tensor for 'J':\")\n",
    "print(letterToTensor('J'))\n",
    "\n",
    "print(\"\\nThis is the dimensionality of the matrix for 'Jones':\")\n",
    "print(lineToTensor('Jones').size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Network\n",
    "\n",
    "Before autograd, creating a recurrent neural network in Torch involved\n",
    "cloning the parameters of a layer over several timesteps. The layers\n",
    "held hidden state and gradients which are now entirely handled by the\n",
    "graph itself. This means you can implement a RNN in a very \"pure\" way,\n",
    "as regular feed-forward layers.\n",
    "\n",
    "This RNN module is just 2 linear layers which operate on an input and hidden state, with\n",
    "a ``LogSoftmax`` layer after the output.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # Define the layers of the model\n",
    "        # These also create the weights where needed\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.h2o = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "        # Set the weights to some initial values\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        # Initialise the weights to be random values in the matrices and zero for the biases\n",
    "        initrange = 0.1\n",
    "        self.i2h.weight.data.uniform_(-initrange, initrange)\n",
    "        self.i2h.bias.data.zero_()\n",
    "        self.h2o.weight.data.uniform_(-initrange, initrange)\n",
    "        self.h2o.bias.data.zero_()\n",
    "        \n",
    "    def initHidden(self):\n",
    "        # Define the initial hidden state\n",
    "        # Here we use an all zero vector\n",
    "        return torch.zeros(1, self.hidden_size)\n",
    "\n",
    "    def forward(self, input_tensor, hidden):\n",
    "        # Given an input, compute the steps defined by the model\n",
    "\n",
    "        # Concatenate the input and hidden vectors\n",
    "        combined = torch.cat((input_tensor, hidden), 1)\n",
    "        \n",
    "        # Apply a linear layer to get the new hidden vector\n",
    "        hidden = self.i2h(combined)\n",
    "\n",
    "        # Apply a linear layer to get the output scores\n",
    "        output = self.h2o(hidden)\n",
    "\n",
    "        # Use softmax to turn the scores into probabilities\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "n_hidden = 128\n",
    "rnn = RNN(n_letters, n_hidden, n_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run a step of this network we need to pass an input (in our case, the\n",
    "Tensor for the current letter) and a previous hidden state (which we\n",
    "initialize as zeros at first). We'll get back the output (probability of\n",
    "each language) and a next hidden state (which we keep for the next\n",
    "step).\n",
    "\n",
    "Note - we haven't trained the model yet, so it's outputs will be random.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_tensor = letterToTensor('A')\n",
    "hidden = torch.zeros(1, n_hidden)\n",
    "\n",
    "output, next_hidden = rnn(input_tensor, hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of efficiency we don't want to be creating a new Tensor for\n",
    "every step, so we will use ``lineToTensor`` instead of\n",
    "``letterToTensor`` and use slices. This could be further optimized by\n",
    "precomputing batches of Tensors.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running 'Albert' through the RNN\n",
      "\n",
      "Likelihood for each label is:\n",
      "tensor([[-2.9116, -2.8747, -2.9155, -2.9160, -2.8815, -2.9172, -2.9344, -2.8851,\n",
      "         -2.8338, -2.9117, -2.8801, -2.9301, -2.8586, -2.8916, -2.8886, -2.8625,\n",
      "         -2.8528, -2.8877]], grad_fn=<LogSoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(\"Running 'Albert' through the RNN\")\n",
    "input_tensor = lineToTensor('Albert')\n",
    "hidden = torch.zeros(1, n_hidden)\n",
    "\n",
    "output, next_hidden = rnn(input_tensor[0], hidden)\n",
    "\n",
    "print(\"\\nLikelihood for each label is:\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the output is a ``<1 x n_categories>`` Tensor, where\n",
    "every item is the likelihood of that category (higher is more likely).\n",
    "\n",
    "# Task 1\n",
    "\n",
    "Write code using pytorch operators to convert these to probabilities by exponentiating them (ie, f(x) = exp(x)). Print the result. You should find that they are all around 5% - 6%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.05438889159769136, 0.05643257859981654, 0.05417808942828872, 0.054150931653534864, 0.05605158567018167, 0.054086197955821855, 0.05316464367000151, 0.05585151422758167, 0.058790161145640646, 0.05438265467331738, 0.05613094222248192, 0.0533902876589362, 0.05735069383087577, 0.05548570081985282, 0.05565527511002109, 0.05712423088286325, 0.05767977030549962, 0.055705747677060234]\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "import math\n",
    "probabilities = []\n",
    "for entry1 in output:\n",
    "    for entry in entry1:\n",
    "        probabilities.append(math.exp(entry))\n",
    "print(str(probabilities))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "Preparing for Training\n",
    "----------------------\n",
    "\n",
    "Before going into training we should make a few helper functions. The\n",
    "first is to interpret the output of the network, which we know to be a\n",
    "likelihood of each category. We can use ``Tensor.topk`` to get the index\n",
    "of the greatest value:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Irish', 8)\n"
     ]
    }
   ],
   "source": [
    "def categoryFromOutput(output):\n",
    "    top_n, top_i = output.topk(1)\n",
    "    category_i = top_i[0].item()\n",
    "    return all_categories[category_i], category_i\n",
    "\n",
    "print(categoryFromOutput(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also want a quick way to get a training example (a name and its\n",
    "language). We use randomness here as training on the same instances in the same order can lead to worse results as we overfit that particular sequence of samples.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are 10 examples of randomly choosing data samples:\n",
      "category = Italian / line = Bulgarelli\n",
      "category = Italian / line = Fortunato\n",
      "category = Czech / line = Kremlacek\n",
      "category = Italian / line = Ferrara\n",
      "category = Arabic / line = Nahas\n",
      "category = Irish / line = Conn\n",
      "category = French / line = Bonfils\n",
      "category = Vietnamese / line = Huynh\n",
      "category = Vietnamese / line = Vuong\n",
      "category = Japanese / line = Royama\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def randomChoice(l):\n",
    "    return l[random.randint(0, len(l) - 1)]\n",
    "\n",
    "def randomChoiceDict(l):\n",
    "    i = random.randint(0, len(l) - 1)\n",
    "    j = 0\n",
    "    for key in l:\n",
    "        if i == j:\n",
    "            return key\n",
    "        j += 1\n",
    "        \n",
    "\n",
    "\n",
    "def randomTrainingExample():\n",
    "    category = randomChoice(all_categories)\n",
    "    line = randomChoice(category_lines[category])\n",
    "    category_tensor = torch.tensor([all_categories.index(category)], dtype=torch.long)\n",
    "    line_tensor = lineToTensor(line)\n",
    "    return category, line, category_tensor, line_tensor\n",
    "\n",
    "print(\"Here are 10 examples of randomly choosing data samples:\")\n",
    "for i in range(10):\n",
    "    category, line, category_tensor, line_tensor = randomTrainingExample()\n",
    "    print('category =', category, '/ line =', line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Network\n",
    "\n",
    "Now all it takes to train this network is show it a bunch of examples,\n",
    "have it make guesses, and tell it if it's wrong.\n",
    "\n",
    "For the loss function ``nn.NLLLoss`` is appropriate, since the last\n",
    "layer of the RNN is ``nn.LogSoftmax``.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each loop of training will:\n",
    "\n",
    "-  Create input and target tensors\n",
    "-  Create a zeroed initial hidden state\n",
    "-  Read each letter in and do the calculation, keeping the hidden state for the next letter\n",
    "-  Compare final output to target\n",
    "-  Back-propagate\n",
    "-  Return the output and loss\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.005 # If you set this too high, it might explode. If too low, it might not learn\n",
    "\n",
    "def train(category_tensor, line_tensor):\n",
    "    hidden = rnn.initHidden()\n",
    "\n",
    "    rnn.zero_grad()\n",
    "\n",
    "    for i in range(line_tensor.size()[0]):\n",
    "        output, hidden = rnn(line_tensor[i], hidden)\n",
    "\n",
    "    loss = criterion(output, category_tensor)\n",
    "    loss.backward()\n",
    "\n",
    "    # Add parameters' gradients to their values, multiplied by learning rate\n",
    "    for p in rnn.parameters():\n",
    "        p.data.add_(p.grad.data, alpha=-learning_rate)\n",
    "\n",
    "    return output, loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we just have to run that with a bunch of examples. Since the\n",
    "``train`` function returns both the output and loss we can print its\n",
    "guesses and also keep track of loss for plotting. Since there are 1000s\n",
    "of examples we print only every ``print_every`` examples, and take an\n",
    "average of the loss.\n",
    "\n",
    "**NOTE:** This can take 5-10 minutes to run.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000 5% (0m 8s) 3.0786 Mojar / English ✗ (Russian)\n",
      "10000 10% (0m 15s) 1.8636 Costa / Czech ✗ (Spanish)\n",
      "15000 15% (0m 23s) 0.6346 Nizhinsky / Russian ✓\n",
      "20000 20% (0m 32s) 1.6420 Fairley / Irish ✗ (English)\n",
      "25000 25% (0m 39s) 0.9758 Tsang / Vietnamese ✗ (Chinese)\n",
      "30000 30% (0m 47s) 1.3830 Nunes / Arabic ✗ (Portuguese)\n",
      "35000 35% (0m 56s) 0.7059 Millar / Scottish ✓\n",
      "40000 40% (1m 3s) 1.1150 Cousineau / French ✓\n",
      "45000 45% (1m 10s) 0.7782 Varvitsiotes / Greek ✓\n",
      "50000 50% (1m 17s) 2.0802 Diefenbach / French ✗ (German)\n",
      "55000 55% (1m 26s) 0.1570 Antoniadis / Greek ✓\n",
      "60000 60% (1m 35s) 1.1237 Kitchen / English ✓\n",
      "65000 65% (1m 43s) 2.0811 Hawlata / Spanish ✗ (Czech)\n",
      "70000 70% (1m 50s) 2.8321 Tonks / Czech ✗ (English)\n",
      "75000 75% (1m 57s) 0.4759 Sotiris / Greek ✓\n",
      "80000 80% (2m 5s) 1.3594 Amello / Portuguese ✗ (Italian)\n",
      "85000 85% (2m 13s) 1.2712 O'Callaghann / German ✗ (Irish)\n",
      "90000 90% (2m 21s) 1.7732 De la fuente / Dutch ✗ (Spanish)\n",
      "95000 95% (2m 29s) 7.0223 Comino / Portuguese ✗ (Greek)\n",
      "100000 100% (2m 38s) 5.0320 Rendon / Irish ✗ (Spanish)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "n_iters = 100000\n",
    "print_every = 5000\n",
    "plot_every = 1000\n",
    "\n",
    "\n",
    "\n",
    "# Keep track of losses for plotting\n",
    "current_loss = 0\n",
    "all_losses = []\n",
    "\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for iter in range(1, n_iters + 1):\n",
    "    category, line, category_tensor, line_tensor = randomTrainingExample()\n",
    "    output, loss = train(category_tensor, line_tensor)\n",
    "    current_loss += loss\n",
    "\n",
    "    # Print ``iter`` number, loss, name and guess\n",
    "    if iter % print_every == 0:\n",
    "        guess, guess_i = categoryFromOutput(output)\n",
    "        correct = '✓' if guess == category else '✗ (%s)' % category\n",
    "        print('%d %d%% (%s) %.4f %s / %s %s' % (iter, iter / n_iters * 100, timeSince(start), loss, line, guess, correct))\n",
    "\n",
    "    # Add current loss avg to list of losses\n",
    "    if iter % plot_every == 0:\n",
    "        all_losses.append(current_loss / plot_every)\n",
    "        current_loss = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the Results\n",
    "\n",
    "Plotting the historical loss from ``all_losses`` shows the network\n",
    "learning.\n",
    "\n",
    "Note that learning is fast and fairly smooth at first, but then the improvements become smaller and more variable.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7ff5b5644850>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(all_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Results\n",
    "\n",
    "Let's see how well the model is doing on the training data. We can get a reasonable estimate with just part of the data, so we'll run 1000 samples through the network with `evaluate()`, which is the same as `train()` minus the backpropagation.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60.5%\n"
     ]
    }
   ],
   "source": [
    "# Just return an output given a line\n",
    "def evaluate(line_tensor):\n",
    "    hidden = rnn.initHidden()\n",
    "\n",
    "    for i in range(line_tensor.size()[0]):\n",
    "        output, hidden = rnn(line_tensor[i], hidden)\n",
    "\n",
    "    return output\n",
    "\n",
    "total = 1000\n",
    "correct = 0\n",
    "# Go through a bunch of examples and record which are correctly guessed\n",
    "for i in range(total):\n",
    "    category, line, category_tensor, line_tensor = randomTrainingExample()\n",
    "    output = evaluate(line_tensor)\n",
    "    guess, guess_i = categoryFromOutput(output)\n",
    "    category_i = all_categories.index(category)\n",
    "    if category_i == guess_i:\n",
    "        correct += 1\n",
    "print(\"{}%\".format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The score should be around 50-60%, which may seem low, but consider how tricky this task can be!\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running on User Input\n",
    "\n",
    "This function shows the output for a sample input you can provide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "> Kummerfeld\n",
      "(-0.75) English\n",
      "(-0.96) German\n",
      "(-2.84) Dutch\n",
      "\n",
      "> Kay\n",
      "(-1.38) Scottish\n",
      "(-1.78) Chinese\n",
      "(-1.98) Vietnamese\n"
     ]
    }
   ],
   "source": [
    "def predict(input_line, n_predictions=3):\n",
    "    print('\\n> %s' % input_line)\n",
    "    with torch.no_grad():\n",
    "        output = evaluate(lineToTensor(input_line))\n",
    "\n",
    "        # Get top N categories\n",
    "        topv, topi = output.topk(n_predictions, 1, True)\n",
    "        predictions = []\n",
    "\n",
    "        for i in range(n_predictions):\n",
    "            value = topv[0][i].item()\n",
    "            category_index = topi[0][i].item()\n",
    "            print('(%.2f) %s' % (value, all_categories[category_index]))\n",
    "            predictions.append([value, all_categories[category_index]])\n",
    "\n",
    "predict('Kummerfeld')\n",
    "predict('Kay')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources\n",
    "\n",
    "For more on RNNs, see:\n",
    "\n",
    "- [Chapter 9 of J+M](https://web.stanford.edu/~jurafsky/slp3/9.pdf)\n",
    "- [Chapter 7, section 6 of E](https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf)\n",
    "-  [The Unreasonable Effectiveness of Recurrent Neural\n",
    "   Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/)_\n",
    "   shows a bunch of real life examples\n",
    "-  [Understanding LSTM\n",
    "   Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)_\n",
    "   is about LSTMs specifically but also informative about RNNs in\n",
    "   general"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2\n",
    "\n",
    "Above, we trained and tested on the same data. That is misleading, because the model saw those examples during training.\n",
    "\n",
    "In this task:\n",
    "1. Modify the data reading process to split the data randomly into a test set (\\~10% of the data) and train set (\\~90% of the data). Train and test again.\n",
    "2. Modify `randomTrainingExample` to sample from your training data. Implement a `randomTestExample` to sample from your test data.\n",
    "3. Create a new instance of the model.\n",
    "4. Train that instance with the training data you created.\n",
    "5. Test it with the test data you created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "#Part 1\n",
    "import random\n",
    "test_set = {}\n",
    "training_set = {}\n",
    "\n",
    "for language in category_lines:\n",
    "    test_set[language] = []\n",
    "    training_set[language] = []\n",
    "    for name in category_lines[language]:\n",
    "        x = random.randint(0, 9)\n",
    "        if x == 9:\n",
    "            test_set[language].append(name)\n",
    "        else:\n",
    "            training_set[language].append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are 10 examples of randomly choosing data samples:\n",
      "category = Czech / line = Semick\n",
      "category = Korean / line = Choi\n",
      "category = Chinese / line = Xuan\n",
      "category = Arabic / line = Awad\n",
      "category = Irish / line = Malone\n",
      "category = Vietnamese / line = Diep\n",
      "category = Spanish / line = Del bosque\n",
      "category = Irish / line = O'Byrne\n",
      "category = Russian / line = Dubrov\n",
      "category = English / line = Rhoades\n"
     ]
    }
   ],
   "source": [
    "def randomTrainingExampleTrain():\n",
    "    category = randomChoiceDict(training_set)\n",
    "    line = randomChoice(training_set[category])\n",
    "    i = 0\n",
    "    for key in training_set:\n",
    "        if key == category:\n",
    "            j = i\n",
    "        i += 1\n",
    "        \n",
    "    category_tensor = torch.tensor([j], dtype=torch.long)\n",
    "    line_tensor = lineToTensor(line)\n",
    "    return category, line, category_tensor, line_tensor\n",
    "\n",
    "print(\"Here are 10 examples of randomly choosing data samples:\")\n",
    "for i in range(10):\n",
    "    category, line, category_tensor, line_tensor = randomTrainingExampleTrain()\n",
    "    print('category =', category, '/ line =', line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Make new model\n",
    "hidden = torch.zeros(1, n_hidden)\n",
    "\n",
    "output, next_hidden = rnn(input_tensor[0], hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000 5% (0m 7s) 1.5260 Suh / Chinese ✗ (Korean)\n",
      "10000 10% (0m 14s) 3.4870 Ganem / Dutch ✗ (Arabic)\n",
      "15000 15% (0m 21s) 0.1272 Gianakopulos / Greek ✓\n",
      "20000 20% (0m 28s) 3.1664 Diefenbach / Irish ✗ (German)\n",
      "25000 25% (0m 35s) 1.7566 Fonda / Spanish ✗ (Italian)\n",
      "30000 30% (0m 43s) 1.4155 An / Korean ✗ (Vietnamese)\n",
      "35000 35% (0m 50s) 0.6240 Wawrzaszek / Polish ✓\n",
      "40000 40% (0m 57s) 0.4407 Peeters / Dutch ✓\n",
      "45000 45% (1m 4s) 1.2320 Abbatantuono / Spanish ✗ (Italian)\n",
      "50000 50% (1m 11s) 0.7831 San / Korean ✓\n",
      "55000 55% (1m 18s) 1.3233 Dufour / French ✓\n",
      "60000 60% (1m 25s) 0.2951 Handal / Arabic ✓\n",
      "65000 65% (1m 33s) 0.1164 Cabrera / Spanish ✓\n",
      "70000 70% (1m 40s) 0.8900 Lefevre / French ✓\n",
      "75000 75% (1m 47s) 2.0620 Jirik / Korean ✗ (Czech)\n",
      "80000 80% (1m 55s) 1.0414 Wen / Korean ✗ (Chinese)\n",
      "85000 85% (2m 2s) 1.0764 Brian / Irish ✓\n",
      "90000 90% (2m 9s) 1.2110 Escott / English ✓\n",
      "95000 95% (2m 17s) 0.2634 Jang / Korean ✓\n",
      "100000 100% (2m 24s) 0.0916 Niemczyk / Polish ✓\n"
     ]
    }
   ],
   "source": [
    "#Train model\n",
    "import time\n",
    "import math\n",
    "\n",
    "n_iters = 100000\n",
    "print_every = 5000\n",
    "plot_every = 1000\n",
    "\n",
    "\n",
    "\n",
    "# Keep track of losses for plotting\n",
    "current_loss = 0\n",
    "all_losses = []\n",
    "\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for iter in range(1, n_iters + 1):\n",
    "    category, line, category_tensor, line_tensor = randomTrainingExampleTrain()\n",
    "    output, loss = train(category_tensor, line_tensor)\n",
    "    current_loss += loss\n",
    "\n",
    "    # Print ``iter`` number, loss, name and guess\n",
    "    if iter % print_every == 0:\n",
    "        guess, guess_i = categoryFromOutput(output)\n",
    "        correct = '✓' if guess == category else '✗ (%s)' % category\n",
    "        print('%d %d%% (%s) %.4f %s / %s %s' % (iter, iter / n_iters * 100, timeSince(start), loss, line, guess, correct))\n",
    "\n",
    "    # Add current loss avg to list of losses\n",
    "    if iter % plot_every == 0:\n",
    "        all_losses.append(current_loss / plot_every)\n",
    "        current_loss = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39.7%\n"
     ]
    }
   ],
   "source": [
    "def selectTrainingExampleTest(i):\n",
    "    j = 0\n",
    "    k = 0\n",
    "    for key in test_set:\n",
    "        for val in test_set[key]:\n",
    "            if j == i:\n",
    "                line = val\n",
    "                category = key\n",
    "                k = j\n",
    "            j += 1\n",
    "        k += 1\n",
    "\n",
    "    category_tensor = torch.tensor(k, dtype=torch.long)\n",
    "    line_tensor = lineToTensor(line)\n",
    "    return category, line, category_tensor, line_tensor\n",
    "\n",
    "total = 1000\n",
    "correct = 0\n",
    "# Go through test set\n",
    "for i in range(total):\n",
    "    category, line, category_tensor, line_tensor = selectTrainingExampleTest(i)\n",
    "    output = evaluate(line_tensor)\n",
    "    guess, guess_i = categoryFromOutput(output)\n",
    "    category_i = all_categories.index(category)\n",
    "    if category_i == guess_i:\n",
    "        correct += 1\n",
    "print(\"{}%\".format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3\n",
    "\n",
    "The model is entirely linear so far. Modify it to be the basic RNN introduced in lecture 4.\n",
    "\n",
    "Note - you can do this task without doing Task 2. It is fine to report results on the training set (as the code below does). If you want to combine task 3 and 4 that's okay too (the staff will have both answers).\n",
    "\n",
    "In the process, also change the weight initialisation to set them to be random values uniformly distributed in the range (-sqrt(k), sqrt(k)) where k is 1/hidden_size.\n",
    "\n",
    "The cells below contains all the key code from above for easier manipulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Data\n",
    "\n",
    "from io import open\n",
    "import glob\n",
    "import os\n",
    "import unicodedata\n",
    "import string\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import numpy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Get files\n",
    "def findFiles(path):\n",
    "    return glob.glob(path)\n",
    "\n",
    "all_letters = string.ascii_letters + \" .,;'\"\n",
    "n_letters = len(all_letters)\n",
    "\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in all_letters\n",
    "    )\n",
    "\n",
    "# Build the category_lines dictionary, a list of names per language\n",
    "category_lines = {}\n",
    "all_categories = []\n",
    "\n",
    "# Read a file and split into lines\n",
    "def readLines(filename):\n",
    "    lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
    "    return [unicodeToAscii(line) for line in lines]\n",
    "\n",
    "for filename in findFiles('data/names/*.txt'):\n",
    "    category = os.path.splitext(os.path.basename(filename))[0]\n",
    "    all_categories.append(category)\n",
    "    lines = readLines(filename)\n",
    "    category_lines[category] = lines\n",
    "\n",
    "n_categories = len(all_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Model and Inference\n",
    "\n",
    "# Find letter index from all_letters, e.g. \"a\" = 0\n",
    "def letterToIndex(letter):\n",
    "    return all_letters.find(letter)\n",
    "\n",
    "# Just for demonstration, turn a letter into a <1 x n_letters> Tensor\n",
    "def letterToTensor(letter):\n",
    "    tensor = torch.zeros(1, n_letters)\n",
    "    tensor[0][letterToIndex(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "# Turn a line into a <line_length x 1 x n_letters>,\n",
    "# or an array of one-hot letter vectors\n",
    "def lineToTensor(line):\n",
    "    tensor = torch.zeros(len(line), 1, n_letters)\n",
    "    for li, letter in enumerate(line):\n",
    "        tensor[li][0][letterToIndex(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # Define the structure of the model\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.h2h = nn.Linear(hidden_size, hidden_size)\n",
    "        self.h2o = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "        # Set the weights to some initial values\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        # Initialise the weights to be random values in the matrices and zero for the biases\n",
    "        initrange = math.sqrt(1/self.hidden_size)\n",
    "        self.i2h.weight.data.uniform_(-initrange, initrange)\n",
    "        self.i2h.bias.data.zero_()\n",
    "        self.h2o.weight.data.uniform_(-initrange, initrange)\n",
    "        self.h2o.bias.data.zero_()\n",
    "        self.h2h.weight.data.uniform_(-initrange, initrange)\n",
    "        self.h2h.bias.data.zero_()\n",
    "        \n",
    "    def initHidden(self):\n",
    "        # Define the initial hidden state for an input as all zeros\n",
    "        return torch.zeros(1, self.hidden_size)\n",
    "\n",
    "    def forward(self, input_tensor, hidden):\n",
    "        # Given an input, compute the steps defined by the model\n",
    "        combined = torch.cat((input_tensor, hidden), 1) #here\n",
    "        hidden = self.i2h(combined)\n",
    "        hidden = self.h2h(hidden)\n",
    "        output = self.h2o(hidden)\n",
    "        #output becomes tanh\n",
    "        output = torch.tanh(output)\n",
    "        \n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "n_hidden = 128\n",
    "rnn = RNN(n_letters, n_hidden, n_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000 5% (0m 11s) 2.8454 Yeon / Irish ✗ (Korean)\n",
      "10000 10% (0m 21s) 1.9780 Salcedo / Italian ✗ (Spanish)\n",
      "15000 15% (0m 32s) 2.2122 Bisset / Dutch ✗ (French)\n",
      "20000 20% (0m 42s) 2.3478 Giang / Vietnamese ✓\n",
      "25000 25% (0m 53s) 1.8024 Reagan / Arabic ✗ (Irish)\n",
      "30000 30% (1m 3s) 1.7146 Sun / Vietnamese ✗ (Korean)\n",
      "35000 35% (1m 14s) 2.1533 Ashley / English ✓\n",
      "40000 40% (1m 24s) 1.5254 Hajjar / Arabic ✓\n",
      "45000 45% (1m 34s) 1.6713 Henderson / Scottish ✓\n",
      "50000 50% (1m 46s) 2.1510 Duarte / French ✗ (Portuguese)\n",
      "55000 55% (1m 56s) 1.6383 Giang / Vietnamese ✓\n",
      "60000 60% (2m 6s) 1.7205 Wei / Chinese ✓\n",
      "65000 65% (2m 17s) 1.7720 Abana / Italian ✗ (Spanish)\n",
      "70000 70% (2m 27s) 2.0041 Beaumont / Dutch ✗ (French)\n",
      "75000 75% (2m 38s) 1.9141 Michaud / Arabic ✗ (French)\n",
      "80000 80% (2m 48s) 1.2204 Vourlis / Greek ✓\n",
      "85000 85% (2m 59s) 1.4512 Kawamata / Japanese ✓\n",
      "90000 90% (3m 9s) 1.5023 La / Vietnamese ✓\n",
      "95000 95% (3m 19s) 1.7392 Nader / German ✗ (Arabic)\n",
      "100000 100% (3m 29s) 2.0395 Doan / Irish ✗ (Vietnamese)\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "\n",
    "n_iters = 100000\n",
    "print_every = 5000\n",
    "plot_every = 1000\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "learning_rate = 0.005 # If you set this too high, it might explode. If too low, it might not learn\n",
    "\n",
    "def categoryFromOutput(output):\n",
    "    top_n, top_i = output.topk(1)\n",
    "    category_i = top_i[0].item()\n",
    "    return all_categories[category_i], category_i\n",
    "\n",
    "def randomChoice(l):\n",
    "    return l[random.randint(0, len(l) - 1)]\n",
    "\n",
    "def randomTrainingExample():\n",
    "    category = randomChoice(all_categories)\n",
    "    line = randomChoice(category_lines[category])\n",
    "    category_tensor = torch.tensor([all_categories.index(category)], dtype=torch.long)\n",
    "    line_tensor = lineToTensor(line)\n",
    "    return category, line, category_tensor, line_tensor\n",
    "\n",
    "def train(category_tensor, line_tensor):\n",
    "    hidden = rnn.initHidden()\n",
    "\n",
    "    rnn.zero_grad()\n",
    "\n",
    "    for i in range(line_tensor.size()[0]):\n",
    "        output, hidden = rnn(line_tensor[i], hidden)\n",
    "\n",
    "    loss = criterion(output, category_tensor)\n",
    "    loss.backward()\n",
    "\n",
    "    # Add parameters' gradients to their values, multiplied by learning rate\n",
    "    for p in rnn.parameters():\n",
    "        p.data.add_(p.grad.data, alpha=-learning_rate)\n",
    "\n",
    "    return output, loss.item()\n",
    "\n",
    "# Keep track of losses for plotting\n",
    "current_loss = 0\n",
    "all_losses = []\n",
    "\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for iter in range(1, n_iters + 1):\n",
    "    category, line, category_tensor, line_tensor = randomTrainingExample()\n",
    "    output, loss = train(category_tensor, line_tensor)\n",
    "    current_loss += loss\n",
    "\n",
    "    # Print ``iter`` number, loss, name and guess\n",
    "    if iter % print_every == 0:\n",
    "        guess, guess_i = categoryFromOutput(output)\n",
    "        correct = '✓' if guess == category else '✗ (%s)' % category\n",
    "        print('%d %d%% (%s) %.4f %s / %s %s' % (iter, iter / n_iters * 100, timeSince(start), loss, line, guess, correct))\n",
    "\n",
    "    # Add current loss avg to list of losses\n",
    "    if iter % plot_every == 0:\n",
    "        all_losses.append(current_loss / plot_every)\n",
    "        current_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44.5%\n"
     ]
    }
   ],
   "source": [
    "# Just return an output given a line\n",
    "def evaluate(line_tensor):\n",
    "    hidden = rnn.initHidden()\n",
    "\n",
    "    for i in range(line_tensor.size()[0]):\n",
    "        output, hidden = rnn(line_tensor[i], hidden)\n",
    "\n",
    "    return output\n",
    "\n",
    "total = 1000\n",
    "correct = 0\n",
    "# Go through a bunch of examples and record which are correctly guessed\n",
    "for i in range(total):\n",
    "    category, line, category_tensor, line_tensor = randomTrainingExample()\n",
    "    output = evaluate(line_tensor)\n",
    "    guess, guess_i = categoryFromOutput(output)\n",
    "    category_i = all_categories.index(category)\n",
    "    if category_i == guess_i:\n",
    "        correct += 1\n",
    "print(\"{}%\".format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
